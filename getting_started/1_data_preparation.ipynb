{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-fiction",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Getting started\n",
    "*Part 1 - Data preparation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-luther",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". lookout-equipment-demo\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets are usually stored here\n",
    "|   |                                    # before they are sent to S3 to allow the\n",
    "|   |                                    # service to reach them\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "├── getting_started/\n",
    "|   ├── 1_data_preparation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "|   ├── 2_dataset_creation.ipynb\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   └── 6_cleanup.ipynb\n",
    "|\n",
    "└── utils/\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-nickel",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Amazon Lookout for Equipment being a very recent service, we need to make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade boto3 tqdm tsia\n",
    "\n",
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-rouge",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from botocore.client import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-junior",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Let's first check if the bucket name is defined, if it exists and if we have access to it from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = config.BUCKET\n",
    "\n",
    "if BUCKET == '<<YOUR_BUCKET>>':\n",
    "    raise Exception('Please update your Amazon S3 bucket name in the config.py file located at the root of this repository and restart the kernel for this notebook.')\n",
    "    \n",
    "else:\n",
    "    # Check access to the configured bucket:\n",
    "    try:\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.meta.client.head_bucket(Bucket=BUCKET)\n",
    "        print(f'Bucket \"{BUCKET}\" exists')\n",
    "        \n",
    "    # Expose error reason:\n",
    "    except ClientError as error:\n",
    "        error_code = int(error.response['Error']['Code'])\n",
    "        if error_code == 403:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" is private: access is forbidden!')\n",
    "            \n",
    "        elif error_code == 404:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA       = os.path.join('..', 'data', 'raw')\n",
    "TMP_DATA       = os.path.join('..', 'data', 'interim')\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed')\n",
    "LABEL_DATA     = os.path.join(PROCESSED_DATA, 'label-data')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')\n",
    "\n",
    "os.makedirs(TMP_DATA,         exist_ok=True)\n",
    "os.makedirs(RAW_DATA,         exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA,   exist_ok=True)\n",
    "os.makedirs(LABEL_DATA,       exist_ok=True)\n",
    "os.makedirs(TRAIN_DATA,       exist_ok=True)\n",
    "\n",
    "ORIGINAL_DATA = 's3://lookout-equipment-getting-started/raw/lookout-equipment.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-selling",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "---\n",
    "Downloading and unzipping the getting started dataset locally on this instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exists = os.path.exists(os.path.join(TMP_DATA, 'sensors-data', 'impeller', 'component2_file1.csv'))\n",
    "raw_data_exists = os.path.exists(os.path.join(RAW_DATA, 'lookout-equipment.zip'))\n",
    "\n",
    "if data_exists:\n",
    "    print('Dataset already available locally, nothing to do.')\n",
    "    print(f'Dataset is available in {TMP_DATA}.')\n",
    "    \n",
    "else:\n",
    "    if not raw_data_exists:\n",
    "        print('Raw data not found, downloading it')\n",
    "        if ORIGINAL_DATA[:5] == 's3://':\n",
    "            !aws s3 cp $ORIGINAL_DATA $RAW_DATA/lookout-equipment.zip\n",
    "        elif ORIGINAL_DATA[:4] == 'http':\n",
    "            !wget $ORIGINAL_DATA -O $RAW_DATA/lookout-equipment.zip\n",
    "        \n",
    "    print('Unzipping raw data...')\n",
    "    !unzip $RAW_DATA/lookout-equipment.zip -d $TMP_DATA\n",
    "    print(f'Done: dataset now available in {TMP_DATA}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-ethics",
   "metadata": {},
   "source": [
    "## Preparing time series data\n",
    "---\n",
    "The time series data are available in the `sensors-data` directory. The industrial asset we are looking at is a [centrifugal pump](https://en.wikipedia.org/wiki/Centrifugal_pump). Such a pump is used to move a fluid by transfering the rotational energy provided by a motor to hydrodynamic energy:\n",
    "\n",
    "<img src=\"assets/centrifugal_pump_annotated.png\" alt=\"Centrifugal pump\" style=\"width: 658px\"/>\n",
    "\n",
    "<div style=\"text-align: center\"><i>Warman centrifugal pump in a coal preparation plant application</i>, by Bernard S. Janse, licensed under <a href=\"https://creativecommons.org/licenses/by/2.5/deed.fr\">CC BY 2.5</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-mozambique",
   "metadata": {},
   "source": [
    "On a pump such as the one displayed in the photo above, the fluid enters at its axis (the black pipe arriving at the \"eye\" of the impeller. Measurements can be taken around the four main components of the centrifugal pump:\n",
    "* The **impeller** (hidden into the round white casing above): this component consists of a series of curved vanes (blades)\n",
    "* The drive **shaft** arriving at the impeller axis (the \"eye\")\n",
    "* The **motor** connected to the impeller by the drive shaft (on the other end of the black pipe above)\n",
    "* The **volute** chamber, offseted on the right compared to the impeller axis: this creates a curved funnel win a decreasing cross-section area towards the pump outlet (at the top of the white pipe above)\n",
    "\n",
    "In the dataset provided, other sensors not located on one of these component are positionned at the **pump** level.\n",
    "\n",
    "**Let's load the content of each CSV file (we have one per component) and build a single CSV file with all the sensors:** we will obtain a dataset with 10 months of data (spanning from `2019-01-01` to `2019-10-27`) for 30 sensors (`Sensor0` to `Sensor29`) with a 1-minute sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loops through each subfolder of the original dataset:\n",
    "sensor_df_list = []\n",
    "tags_description_dict = dict()\n",
    "for root, dirs, files in os.walk(os.path.join(TMP_DATA, 'sensors-data')):\n",
    "    # Reads each file and set the first column as an index:\n",
    "    for f in files:\n",
    "        print('Processing:', os.path.join(root, f))\n",
    "        df = pd.read_csv(os.path.join(root, f))\n",
    "        df = df.set_index('Timestamp')\n",
    "        sensor_df_list.append(df)\n",
    "        \n",
    "        component = root.split('/')[-1]\n",
    "        current_sensors = df.columns.tolist()\n",
    "        current_sensors = dict(zip(current_sensors, [component] * len(current_sensors)))\n",
    "        tags_description_dict = {**tags_description_dict, **current_sensors}\n",
    "        \n",
    "# Concatenate into a single dataframe:\n",
    "equipment_df = pd.concat(sensor_df_list, axis='columns')\n",
    "equipment_df = equipment_df.reset_index()\n",
    "equipment_df['Timestamp'] = pd.to_datetime(equipment_df['Timestamp'])\n",
    "equipment_df = equipment_df[[\n",
    "    'Timestamp', 'Sensor0', 'Sensor1', 'Sensor2', 'Sensor3', 'Sensor4',\n",
    "    'Sensor5', 'Sensor6', 'Sensor7', 'Sensor8', 'Sensor9', 'Sensor10',\n",
    "    'Sensor11', 'Sensor24', 'Sensor25', 'Sensor26', 'Sensor27', 'Sensor28',\n",
    "    'Sensor29', 'Sensor12', 'Sensor13', 'Sensor14', 'Sensor15', 'Sensor16',\n",
    "    'Sensor17', 'Sensor18', 'Sensor19', 'Sensor20', 'Sensor21', 'Sensor22',\n",
    "    'Sensor23'\n",
    "]]\n",
    "\n",
    "# Register a component for each sensor:\n",
    "tags_description_df = pd.DataFrame.from_dict(tags_description_dict, orient='index')\n",
    "tags_description_df = tags_description_df.reset_index()\n",
    "tags_description_df.columns = ['Tag', 'Component']\n",
    "\n",
    "print(equipment_df.shape)\n",
    "equipment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.makedirs(os.path.join(TRAIN_DATA, 'centrifugal-pump'), exist_ok=True)\n",
    "equipment_fname = os.path.join(TRAIN_DATA, 'centrifugal-pump', 'sensors.csv')\n",
    "equipment_df.to_csv(equipment_fname, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-history",
   "metadata": {},
   "source": [
    "Let's also persist the tags description file as it will be useful when analyzing the model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_description_fname = os.path.join(TMP_DATA, 'tags_description.csv')\n",
    "tags_description_df.to_csv(tags_description_fname, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-creek",
   "metadata": {},
   "source": [
    "## Loading label data\n",
    "---\n",
    "This dataset contains synthetically generated anomalies over different periods of time. Labels are stored as time ranges with a start and end timestamp. Each label is a period of time where we know some anomalous behavior happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fname = os.path.join(TMP_DATA, 'label-data', 'labels.csv')\n",
    "labels_df = pd.read_csv(label_fname, header=None)\n",
    "labels_df.to_csv(os.path.join(PROCESSED_DATA, 'label-data', 'labels.csv'), index=None, header=None)\n",
    "labels_df.columns = ['start', 'end']\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-essence",
   "metadata": {},
   "source": [
    "## Uploading data to Amazon S3\n",
    "---\n",
    "Let's now load our training data and labels to Amazon S3, so that Lookout for Equipment can access them to train and evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_path = f's3://{BUCKET}/training-data/centrifugal-pump/sensors.csv'\n",
    "!aws s3 cp $equipment_fname $train_s3_path\n",
    "\n",
    "label_s3_path = f's3://{BUCKET}/label-data/labels.csv'\n",
    "!aws s3 cp $label_fname $label_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-occasion",
   "metadata": {},
   "source": [
    "## (Optional) Data exploration\n",
    "---\n",
    "This section is optional and just aim at giving you a quick overview about what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import tsia\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('Solarize_Light2')\n",
    "plt.rcParams['lines.linewidth'] = 0.5\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipment_df['Timestamp'] = pd.to_datetime(equipment_df['Timestamp'])\n",
    "equipment_df = equipment_df.set_index('Timestamp')\n",
    "print(equipment_df.shape)\n",
    "equipment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = equipment_df.index.min()\n",
    "end = equipment_df.index.max()\n",
    "\n",
    "print(start, '|', end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-washer",
   "metadata": {},
   "source": [
    "**Let's plot the first signal and the associated labels:** the `plot_timeseries` function is a utility function you can use to plot a signal and the associated labels on the same figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'Sensor0'\n",
    "tag_df = equipment_df.loc[start:end, [tag]]\n",
    "tag_df.columns = ['Value']\n",
    "\n",
    "fig1, axes = lookout.plot_timeseries(\n",
    "    tag_df, \n",
    "    tag, \n",
    "    fig_width=20, \n",
    "    labels_df=labels_df,\n",
    "    custom_grid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-nylon",
   "metadata": {},
   "source": [
    "**Run the following cell to get an overview of every signals in the dataset:** colors are allocated to each sensor according to the component it's associated to. This generates a big matplotlib picture in memory. On smaller instances, this can lead to some *out of memory* issues. Upgrade to a bigger instance, or clean up the memory of the instances if you have other notebooks running in parallel to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "features = equipment_df.columns.tolist()\n",
    "for sensor in features:\n",
    "    df_list.append(equipment_df[[sensor]])\n",
    "    \n",
    "fig2 = tsia.plot.plot_multivariate_timeseries(\n",
    "    timeseries_list=df_list,\n",
    "    tags_list=features,\n",
    "    tags_description_df=tags_description_df,\n",
    "    tags_grouping_key='Component',\n",
    "    num_cols=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-rwanda",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "In this notebook, you downloaded the getting started dataset and prepared it for ingestion in Amazon Lookout for Equipment.\n",
    "\n",
    "You also had a quick overview of the dataset with basic timeseries visualization.\n",
    "\n",
    "You uploaded the training time series data and the anomaly labels to Amazon S3: in the next notebook of this getting started, you will be acquainted with the Amazon Lookout for Equipment API to create your first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup, might be necessary on smaller instances:\n",
    "import gc\n",
    "del fig1, fig2, equipment_df\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
